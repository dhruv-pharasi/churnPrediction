{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, 'churn' refers to the people leaving a bank, i.e., people closing their account. The aim of this project is to find the best classification model to predict when a customer is likely to churn. \n",
    "\n",
    "For a business, this project is important because the cost to attract a new customer is much higher than the cost to retain a customer. This means that customer retention is cost-efficient, which businesses need to maximize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset for this project was taken from Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required modules\n",
    "\n",
    "# Pandas and Matplotlib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining the dataset\n",
    "df = pd.read_csv(\"churn_dataset.csv\").drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)    # RowNumber, CustomerId, and Surname don't affect churn\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining labels\n",
    "df['Exited'].value_counts()\n",
    "\n",
    "# 0 -> Exited their bank (churn)\n",
    "# 1 -> Didn't exit their bank (didn't churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for NaN values\n",
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking correlation of features with the label 'Exited'\n",
    "df.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most significant features are: Age, Balance, IsActiveMember, because they have the highest correlation with the label 'Exited'. \n",
    "\n",
    "**Age:** Typically, older customers exhibit a higher likelihood of remaining with their bank compared to their younger customers.\n",
    "\n",
    "**Balance:** Individuals with substantial account balances are generally less inclined to leave due to concerns about potential financial losses or the inconvenience associated with transferring funds.\n",
    "\n",
    "**IsActiveMember:** Active customers might be constantly on the hunt for banks with the best offers, and hence are more likely to switch banks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 3 features will be used to create a model that predicts churn. However, the features need to be normalized before using them to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing distribution of Age\n",
    "plt.hist(df['Age'], bins=15)\n",
    "plt.title(\"Age Distribution\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows majority of customers in the dataset are aged between 30 and 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing distribution of Balance\n",
    "plt.hist(df['Balance'], bins=15)\n",
    "plt.title(\"Account Balance Distribution\")\n",
    "plt.xlabel(\"Account Balance\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of accounts in the dataset have 0 balance. Other accounts seem to be normally distributed, as they form a bell curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting features on a 3D graph\n",
    "age = df['Age'].to_list()\n",
    "balance = df['Balance'].to_list()\n",
    "is_active_member = df['IsActiveMember'].to_list()\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(age, balance, is_active_member, c='blue', marker='o', label='Data Points')\n",
    "\n",
    "plt.title(\"Features\")\n",
    "\n",
    "ax.set_xlabel('Age', labelpad=15)\n",
    "ax.set_ylabel('Balance', labelpad=15)\n",
    "ax.set_zlabel('IsActiveMember')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3D graph does not reveal much about the distribution of data, other than two large clusters formed. Probably because there are 10000 data points plotted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting labels\n",
    "labels_df = df[['Exited']]\n",
    "\n",
    "# Normalizing features to prevent Balance dominating Age and IsActiveMember \n",
    "scaler = MinMaxScaler()\n",
    "normalized_features = scaler.fit_transform(df[['Age', 'Balance', 'IsActiveMember']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data (80/20)\n",
    "X_test, X_train, Y_test, Y_train = train_test_split(normalized_features, labels_df, test_size=0.2, random_state=4353)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "log_reg.fit(X_train, Y_train)\n",
    "\n",
    "predictions = log_reg.predict(X_test)\n",
    "\n",
    "log_reg_accuracy = accuracy_score(Y_test, predictions)\n",
    "\n",
    "f\"Accuracy: {log_reg_accuracy}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors\n",
    "\n",
    "# Selecting optimal number of neighbors (Hyperparameter tuning)\n",
    "knn_accuracy = []\n",
    "\n",
    "for n in range(1, 30):\n",
    "    knn = KNeighborsClassifier(n_neighbors=n)\n",
    "    knn.fit(X_train, Y_train)\n",
    "    predictions = knn.predict(X_test)\n",
    "    knn_accuracy.append(accuracy_score(Y_test, predictions))\n",
    "\n",
    "# Visualizing the accuracy\n",
    "plt.scatter(list(range(1, 30)), knn_accuracy)\n",
    "plt.title(\"RandomForest Trees Optimization\")\n",
    "plt.xlabel(\"n\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Maximum accuracy: {max(knn_accuracy)}\")\n",
    "\n",
    "print(f\"Optimal value for n (number of neighbors): {knn_accuracy.index(max(knn_accuracy)) + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "# Selecting optimal number of Trees in Forest (Hyperparameter tuning)\n",
    "rForest_accuracy = []\n",
    "\n",
    "for n in range(1, 25):\n",
    "    r_forest = RandomForestClassifier(n_estimators=n)\n",
    "    r_forest.fit(X_train, Y_train)\n",
    "    predictions = r_forest.predict(X_test)\n",
    "    rForest_accuracy.append(accuracy_score(Y_test, predictions))\n",
    "\n",
    "# Visualizing the accuracy\n",
    "plt.scatter(list(range(1, 25)), rForest_accuracy)\n",
    "plt.title(\"RandomForest Trees Optimization\")\n",
    "plt.xlabel(\"n\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Maximum accuracy: {max(rForest_accuracy)}\")\n",
    "\n",
    "f\"Optimal value for n (number of Trees): {rForest_accuracy.index(max(rForest_accuracy)) + 1}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "svc = SVC()\n",
    "\n",
    "svc.fit(X_train, Y_train)\n",
    "\n",
    "predictions = svc.predict(X_test)\n",
    "\n",
    "svc_accuracy = accuracy_score(Y_test, predictions)\n",
    "\n",
    "f\"SVC Accuracy: {svc_accuracy}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the predictions of all 4 models (Voting Ensemble) might increase accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting Ensemble\n",
    "\n",
    "# Creating individual models, using optimized hyperparameters previously calculated\n",
    "ensemble_log_reg = LogisticRegression()\n",
    "ensemble_rForest = RandomForestClassifier(n_estimators=rForest_accuracy.index(max(rForest_accuracy)) + 1)\n",
    "ensemble_knn = KNeighborsClassifier(n_neighbors=knn_accuracy.index(max(knn_accuracy)) + 1)\n",
    "ensemble_svc = SVC()\n",
    "\n",
    "# Creating ensemble\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[('lr', ensemble_log_reg), ('rf', ensemble_rForest), ('knn', ensemble_knn), ('svc', ensemble_svc)], \n",
    "    voting='hard', \n",
    "    weights=[1,1,2,1])\n",
    "\n",
    "ensemble.fit(X_train, Y_train)\n",
    "\n",
    "predictions = ensemble.predict(X_test)\n",
    "\n",
    "ensemble_accuracy = accuracy_score(Y_test, predictions)\n",
    "\n",
    "f\"Voting Ensemble Accuracy: {ensemble_accuracy}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing the 5 models\n",
    "model_names = ['Logistic\\nRegression', 'K-Nearest\\nNeighbors', 'Random\\nForest', 'Support Vector\\nMachine', 'Voting\\nEnsemble']\n",
    "model_accuracies = [log_reg_accuracy, max(knn_accuracy), max(rForest_accuracy), svc_accuracy, ensemble_accuracy]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(model_names, model_accuracies)\n",
    "plt.title('Model Assessment')\n",
    "plt.xlabel('Model', labelpad=10)\n",
    "plt.ylabel('Accuracy', labelpad=10)\n",
    "\n",
    "# Add value annotations to the bars\n",
    "for idx, acc in enumerate(model_accuracies):\n",
    "    plt.text(idx, acc, str(acc), ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors and Voting Ensemble models seem to have a slight edge over the other three models when predicting churn. \n",
    "\n",
    "A voting ensemble could be improved by using more advanced classification algorithms such as Deep Neural Networks (DNNs), as its capable of extracting much complex patterns from the current dataset.\n",
    "\n",
    "On the other hand, K-Nearest Neighbors is a simpler model and could be improved by supplying it with more data to train it. \n",
    "\n",
    "Overall, for banks, availability of data should not be a problem, so they could use any of the two models to predict churn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.) **Addressing Imbalanced Data:** To enhance model accuracy, addressing the dataset's class imbalance is crucial. The limited 'not Exited' labels might have affected the model's ability to predict customer retention accurately. \n",
    "\n",
    "2.) **Missing Key Feature:** The dataset lacked a crucial feature â€” 'Interest Rate', for savings or loans. This omission is significant because interest rates can strongly influence churn. Banks offering higher savings rates or lower loan rates tend to retain customers more effectively. The absence of this feature could have limited model accuracy.\n",
    "\n",
    "3.) **Enhanced Model Assessment:** The utilization of k-fold cross-validation offers a more comprehensive evaluation of model accuracy. This iterative training and testing approach is well-suited for robust assessments, surpassing the limitations of a single train-test split. It provides a more reliable measure of model performance "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
